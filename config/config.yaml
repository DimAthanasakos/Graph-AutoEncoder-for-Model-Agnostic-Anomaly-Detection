# config file

# Size of labeled data to load (i.e. number of jets).  
n_train: 25000
n_val:   2500
n_test:  2500

n_part: [40] # how many particles to consider in each graph/jet.
subjets: 0 # 0: hadrons (False), 1: subjets (True)

models: ['EdgeNet_edge']

EdgeNet: 
  graph_types: ['fully_connected']  # 'laman', 'fully_connected'
  batch_size: 512
  epochs: 80

  learning_rate: 0.005 # 0.01 seems to be the best for the EdgeNet
  patience: 3
  lossname: 'MSE'
  n_sig: 10000
  n_bkg: 10000
  input_dim:      3
  
EdgeNet_edge: 
  graph_types: ['fully_connected']  # 'laman', 'fully_connected'
  batch_size: 128
  epochs: 70

  learning_rate: 0.01 # 0.01 seems to be the best for the EdgeNet
  patience: 3
  lossname: 'MSE'
  n_sig: 10000
  n_bkg: 10000
  input_dim:      1
  pair_input_dim: 3

HybridEdgeNet: 
  graph_types: ['fully_connected']  # 'laman', 'fully_connected'
  batch_size: 512
  epochs: 50

  learning_rate: 0.005 # 0.01 seems to be the best for the EdgeNet
  patience: 3
  lossname: 'MSE'
  n_sig: 10000
  n_bkg: 10000
  input_dim:      3


GATAE:
  graph_types: ['fully_connected']
  heads: 2
  batch_size: 512
  epochs: 7

  learning_rate: 0.001
  patience: 3
  lossname: 'MSE'
  n_sig: 10000
  n_bkg: 10000

transformer:
  batch_size: 512
  epochs: 20

  learning_rate: 0.005 # ~0.001 seems to be the best for the transformer
  patience: 1
  lossname: 'MSE'
  n_sig: 20000
  n_bkg: 20000
  input_dim:      3
  pair_input_dim: 3
  

transformer_graph:
  batch_size: 512
  epochs: 10

  learning_rate: 0.001 # 0.001 seems to be the best for the transformer
  patience: 1
  lossname: 'MSE'
  n_sig: 10000
  n_bkg: 10000

  graph: 'laman_knn_graph'
  input_dim:      3
  pair_input_dim: 3
  add_angles: 0 
